# %% [markdown]
# # –ë–∞–Ω–∫–æ–≤—Å–∫–∏–π –º–∞—Ä–∫–µ—Ç–∏–Ω–≥: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∫–∏ –Ω–∞ –¥–µ–ø–æ–∑–∏—Ç
# ## Bank Marketing Data Analysis
# 
# ![Bank Marketing](https://img.icons8.com/color/96/000000/bank.png)
# 
# **–¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞**: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø–æ–¥–ø–∏—à–µ—Ç—Å—è –ª–∏ –∫–ª–∏–µ–Ω—Ç –Ω–∞ —Ç–µ—Ä–º–∏–Ω–Ω—ã–π –¥–µ–ø–æ–∑–∏—Ç, –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
# 
# **–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö**: [Bank Marketing Dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)
# 
# ---

# %% [markdown]
# ## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
# 1. [–ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫](#1)
# 2. [–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö](#2)
# 3. [–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑](#3)
# 4. [–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã](#4)
#    - [–í–æ–ø—Ä–æ—Å 1](#4.1)
#    - [–í–æ–ø—Ä–æ—Å 2](#4.2)
#    - [–í–æ–ø—Ä–æ—Å 3](#4.3)
#    - [–í–æ–ø—Ä–æ—Å 4](#4.4)
#    - [–í–æ–ø—Ä–æ—Å 5](#4.5)
#    - [–í–æ–ø—Ä–æ—Å 6](#4.6)
# 5. [–ó–∞–∫–ª—é—á–µ–Ω–∏–µ](#5)

# %% [markdown]
# <a id="1"></a>
# ## 1. –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª–µ–π
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
print("‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")

# %% [markdown]
# <a id="2"></a>
# ## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω—ã–π –æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö

# %%
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
try:
    df = pd.read_csv('bank-full.csv', sep=';')
    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
except FileNotFoundError:
    print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª 'bank-full.csv' –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    print("üîÑ –°–æ–∑–¥–∞—é –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")
    import io
    sample_data = """age;job;marital;education;default;balance;housing;loan;contact;day;month;duration;campaign;pdays;previous;poutcome;y
    30;unemployed;married;primary;no;1787;no;no;cellular;19;oct;79;1;-1;0;unknown;no
    33;services;married;secondary;no;4789;yes;yes;cellular;11;may;220;1;339;4;failure;no"""
    df = pd.read_csv(io.StringIO(sample_data), sep=';')

# %%
# –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö
print("üîç –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö:")
df.head()

# %%
# –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö
print("üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {df.shape[0]}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {df.shape[1]}")
print("\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
print(df.dtypes.value_counts())
print("\n–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
print(df.isnull().sum().sum())

# %% [markdown]
# <a id="3"></a>
# ## 3. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö

# %%
# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print("üìà –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
df.describe()

# %%
# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
target_counts = df['y'].value_counts()
plt.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (y)')

plt.subplot(1, 2, 2)
sns.countplot(data=df, x='y')
plt.title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º')
plt.tight_layout()
plt.show()

print(f"üéØ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {target_counts['no']/len(df)*100:.1f}% 'no' vs {target_counts['yes']/len(df)*100:.1f}% 'yes'")

# %% [markdown]
# <a id="4"></a>
# ## 4. –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

# %% [markdown]
# <a id="4.1"></a>
# ### üîç –í–æ–ø—Ä–æ—Å 1: –ö–∞–∫–æ–µ —Å–∞–º–æ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å—Ç–æ–ª–±—Ü–∞ education?

# %%
# –ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–ª–±—Ü–∞ education
education_analysis = df['education'].value_counts()

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.countplot(data=df, y='education', order=education_analysis.index)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è')
plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')

plt.subplot(1, 2, 2)
plt.pie(education_analysis.values, labels=education_analysis.index, autopct='%1.1f%%', startangle=90)
plt.title('–î–æ–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è')

plt.tight_layout()
plt.show()

most_common_education = education_analysis.index[0]
print(f"‚úÖ –û—Ç–≤–µ—Ç 1: –°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç–æ–ª–±—Ü–µ education - '{most_common_education}'")
print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {education_analysis.iloc[0]} ({education_analysis.iloc[0]/len(df)*100:.1f}%)")

# %% [markdown]
# <a id="4.2"></a>
# ### üîç –í–æ–ø—Ä–æ—Å 2: –ö–∞–∫–∏–µ –¥–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏–º–µ—é—Ç –Ω–∞–∏–±–æ–ª—å—à—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é?

# %%
# –í—ã–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
numeric_columns = df.select_dtypes(include=[np.number]).columns
print(f"üî¢ –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(numeric_columns)}): {list(numeric_columns)}")

# %%
# –°—Ç—Ä–æ–∏–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
plt.figure(figsize=(12, 10))
correlation_matrix = df[numeric_columns].corr()

mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,
            square=True, fmt='.2f', cbar_kws={"shrink": .8})
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontsize=16, pad=20)
plt.tight_layout()
plt.show()

# %%
# –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (–∏—Å–∫–ª—é—á–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å)
corr_matrix = df[numeric_columns].corr().abs()
np.fill_diagonal(corr_matrix.values, 0)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å

max_corr_value = corr_matrix.max().max()
max_corr_pair = corr_matrix.stack().idxmax()

print(f"‚úÖ –û—Ç–≤–µ—Ç 2: –ù–∞–∏–±–æ–ª—å—à–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É '{max_corr_pair[0]}' –∏ '{max_corr_pair[1]}'")
print(f"   –ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏: {max_corr_value:.4f}")

# %% [markdown]
# <a id="4.3"></a>
# ### üîç –í–æ–ø—Ä–æ—Å 3: –ö–∞–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –∏–º–µ–µ—Ç –Ω–∞–∏–±–æ–ª—å—à—É—é –≤–∑–∞–∏–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é?

# %%
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
categorical_columns = df.select_dtypes(include=['object']).columns.drop('y')
print(f"üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(categorical_columns)}): {list(categorical_columns)}")

# %%
# –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ MI
df_encoded = df.copy()
le = LabelEncoder()

for col in categorical_columns:
    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))

# –ö–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
df_encoded['y'] = le.fit_transform(df_encoded['y'])

# %%
# –í—ã—á–∏—Å–ª—è–µ–º –≤–∑–∞–∏–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
mi_scores = mutual_info_classif(df_encoded[categorical_columns], df_encoded['y'], random_state=42)
mi_df = pd.DataFrame({'Feature': categorical_columns, 'MI_Score': mi_scores})
mi_df = mi_df.sort_values('MI_Score', ascending=False)

# %%
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
plt.figure(figsize=(12, 8))
bars = plt.barh(mi_df['Feature'], mi_df['MI_Score'], color='lightcoral')
plt.xlabel('Mutual Information Score')
plt.title('–í–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π', fontsize=14)
plt.gca().invert_yaxis()

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
for bar, value in zip(bars, mi_df['MI_Score']):
    plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2, 
             f'{value:.3f}', ha='left', va='center')

plt.tight_layout()
plt.show()

max_mi_feature = mi_df.iloc[0]['Feature']
max_mi_score = mi_df.iloc[0]['MI_Score']

print(f"‚úÖ –û—Ç–≤–µ—Ç 3: –ù–∞–∏–±–æ–ª—å—à–∞—è –≤–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —É –ø—Ä–∏–∑–Ω–∞–∫–∞ '{max_mi_feature}'")
print(f"   –ó–Ω–∞—á–µ–Ω–∏–µ MI: {max_mi_score:.4f}")

# %% [markdown]
# <a id="4.4"></a>
# ### üîç –í–æ–ø—Ä–æ—Å 4: –¢–æ—á–Ω–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ

# %%
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
X = df.drop('y', axis=1)
y = df['y'].map({'yes': 1, 'no': 0})

# One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
X_encoded = pd.get_dummies(X, drop_first=True)

# %%
# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
X_temp, X_test, y_temp, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

print("üìä –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫:")
print(f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è: {X_train.shape}")
print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è: {X_val.shape}")
print(f"   –¢–µ—Å—Ç–æ–≤–∞—è: {X_test.shape}")

# %%
# –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
base_model = LogisticRegression(
    solver='liblinear',
    C=1.0,
    max_iter=1000,
    random_state=42
)

base_model.fit(X_train, y_train)

# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
y_val_pred = base_model.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)

print(f"‚úÖ –û—Ç–≤–µ—Ç 4: –¢–æ—á–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ: {val_accuracy:.2f}")

# %%
# –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
print("\nüìä –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏:")
print(classification_report(y_val, y_val_pred, target_names=['No', 'Yes']))

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_val, y_val_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicted No', 'Predicted Yes'],
            yticklabels=['Actual No', 'Actual Yes'])
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Validation Set)')
plt.show()

# %% [markdown]
# <a id="4.5"></a>
# ### üîç –í–æ–ø—Ä–æ—Å 5: –ö–∞–∫–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –∏–º–µ–µ—Ç –Ω–∞–∏–º–µ–Ω—å—à—É—é —Ä–∞–∑–Ω–∏—Ü—É –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏?

# %%
# Feature Elimination Analysis
features_to_test = ['age', 'balance', 'marital', 'previous']
elimination_results = {}

print("üîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è:")
print(f"–ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.4f}\n")

for feature in features_to_test:
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    if feature in ['age', 'balance', 'previous']:
        cols_to_drop = [feature]
    else:
        cols_to_drop = [col for col in X_train.columns if col.startswith(feature + '_')]
    
    # –°–æ–∑–¥–∞–µ–º –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø—Ä–∏–∑–Ω–∞–∫–∞
    X_train_reduced = X_train.drop(columns=cols_to_drop)
    X_val_reduced = X_val.drop(columns=cols_to_drop)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –±–µ–∑ –ø—Ä–∏–∑–Ω–∞–∫–∞
    model_reduced = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)
    model_reduced.fit(X_train_reduced, y_train)
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
    y_val_pred_reduced = model_reduced.predict(X_val_reduced)
    accuracy_reduced = accuracy_score(y_val, y_val_pred_reduced)
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É
    difference = val_accuracy - accuracy_reduced
    elimination_results[feature] = difference
    
    print(f"–ë–µ–∑ '{feature}': —Ç–æ—á–Ω–æ—Å—Ç—å = {accuracy_reduced:.4f}, —Ä–∞–∑–Ω–∏—Ü–∞ = {difference:.4f}")

# %%
# –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–∑–Ω–∞–∫ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –≤–ª–∏—è–Ω–∏–µ–º
min_impact_feature = min(elimination_results, key=lambda x: abs(elimination_results[x]))
min_impact_value = elimination_results[min_impact_feature]

print(f"‚úÖ –û—Ç–≤–µ—Ç 5: –ù–∞–∏–º–µ–Ω—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫ '{min_impact_feature}'")
print(f"   –†–∞–∑–Ω–∏—Ü–∞ –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏: {min_impact_value:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
plt.figure(figsize=(10, 6))
features = list(elimination_results.keys())
differences = list(elimination_results.values())

bars = plt.bar(features, differences, color=['skyblue' if x != min_impact_feature else 'lightcoral' for x in features])
plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
plt.ylabel('–†–∞–∑–Ω–∏—Ü–∞ –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏')
plt.title('–í–ª–∏—è–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
plt.xticks(rotation=45)

for bar, diff in zip(bars, differences):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.001 if bar.get_height() >= 0 else -0.003), 
             f'{diff:.4f}', ha='center', va='bottom' if bar.get_height() >= 0 else 'top')

plt.tight_layout()
plt.show()

# %% [markdown]
# <a id="4.6"></a>
# ### üîç –í–æ–ø—Ä–æ—Å 6: –ö–∞–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ C –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∞–∏–ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏?

# %%
# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ C
C_values = [0.01, 0.1, 1, 10]
accuracy_results = {}

print("üéØ –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ C:")

plt.figure(figsize=(12, 5))

for i, C in enumerate(C_values, 1):
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    model_reg = LogisticRegression(
        solver='liblinear',
        C=C,
        max_iter=1000,
        random_state=42
    )
    model_reg.fit(X_train, y_train)
    
    # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞
    y_val_pred_reg = model_reg.predict(X_val)
    accuracy_reg = accuracy_score(y_val, y_val_pred_reg)
    accuracy_results[C] = accuracy_reg
    
    print(f"C = {C:5.2f}: —Ç–æ—á–Ω–æ—Å—Ç—å = {accuracy_reg:.4f}")

# %%
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
best_C = max(accuracy_results, key=accuracy_results.get)
best_accuracy = accuracy_results[best_C]

plt.figure(figsize=(10, 6))
plt.plot(list(accuracy_results.keys()), list(accuracy_results.values()), 
         marker='o', linewidth=2, markersize=8, color='teal')
plt.xscale('log')
plt.xlabel('–ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ C')
plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
plt.title('–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ C', fontsize=14)
plt.grid(True, alpha=0.3)

# –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
plt.axvline(x=best_C, color='red', linestyle='--', alpha=0.7, label=f'–õ—É—á—à–µ–µ C = {best_C}')
plt.legend()

# –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
for C, acc in accuracy_results.items():
    plt.annotate(f'{acc:.4f}', (C, acc), xytext=(5, 5), textcoords='offset points',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))

plt.tight_layout()
plt.show()

print(f"‚úÖ –û—Ç–≤–µ—Ç 6: –õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ C = {best_C}")
print(f"   –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.4f}")

# %% [markdown]
# <a id="5"></a>
# ## 5. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –∏ –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

# %%
# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("=" * 60)
print("üéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")
print("=" * 60)

results_summary = {
    "–í–æ–ø—Ä–æ—Å 1": f"–°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ education: '{most_common_education}'",
    "–í–æ–ø—Ä–æ—Å 2": f"–ù–∞–∏–±–æ–ª—å—à–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: '{max_corr_pair[0]}' –∏ '{max_corr_pair[1]}' ({max_corr_value:.4f})",
    "–í–æ–ø—Ä–æ—Å 3": f"–ù–∞–∏–±–æ–ª—å—à–∞—è –≤–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: '{max_mi_feature}' ({max_mi_score:.4f})",
    "–í–æ–ø—Ä–æ—Å 4": f"–¢–æ—á–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {val_accuracy:.2f}",
    "–í–æ–ø—Ä–æ—Å 5": f"–ù–∞–∏–º–µ–Ω—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ: '{min_impact_feature}' (—Ä–∞–∑–Ω–∏—Ü–∞: {min_impact_value:.4f})",
    "–í–æ–ø—Ä–æ—Å 6": f"–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ C: {best_C} (—Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.4f})"
}

for question, answer in results_summary.items():
    print(f"üîπ {question}: {answer}")

print("=" * 60)

# %%
# –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
metrics_data = {
    '–ú–µ—Ç—Ä–∏–∫–∞': ['–ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å', '–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å', '–í–ª–∏—è–Ω–∏–µ –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞'],
    '–ó–Ω–∞—á–µ–Ω–∏–µ': [val_accuracy, best_accuracy, abs(min_impact_value)],
    '–û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å': ['–ë–∞–∑–æ–≤–∞—è', '–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è', '–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ']
}

metrics_df = pd.DataFrame(metrics_data)

plt.figure(figsize=(10, 6))
bars = plt.barh(metrics_df['–ú–µ—Ç—Ä–∏–∫–∞'], metrics_df['–ó–Ω–∞—á–µ–Ω–∏–µ'], 
                color=['lightblue', 'lightgreen', 'lightcoral'])
plt.xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏')
plt.title('–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏', fontsize=14)

for bar, value in zip(bars, metrics_df['–ó–Ω–∞—á–µ–Ω–∏–µ']):
    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
             f'{value:.4f}', va='center')

plt.tight_layout()
plt.show()

# %%
print("""
üìå –í–´–í–û–î–´:
‚Ä¢ –ú–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø–æ–∫–∞–∑–∞–ª–∞ —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
‚Ä¢ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–∏–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏
‚Ä¢ –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤—ã—è–≤–∏–ª –Ω–∞–∏–±–æ–ª–µ–µ –∏ –Ω–∞–∏–º–µ–Ω–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
‚Ä¢ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö
""")

print("üéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
